{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99f2c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.3.0)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/c6/99/ed35197a158f1fdc2fe7c3680e9c70d0128f662e1fee4ed495f4b5e13db0/scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/7d/48/7e9581b476df76aaf9ee182888d15322e77c38b0bbbd5e80160ba0bddd4c/spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: gensim in c:\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gisel\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/1e/e8/685f47e0d754320684db4425a0967f7d3fa70126bffd76110b7009a0090f/joblib-1.5.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/84/a4/b249b042f5afe34d14ada2dc4afc777e883c15863296756179652e081c44/murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/dd/fe/3ee37d02ca4040f2fb22d34eb415198f955862b5dd47eee01df4c8f5454c/cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/12/a2285b78bd097a1e53fb90a1743bc8ce0d35e5b65b6853f3b3c47da398ca/preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.4.0,>=8.3.4 from https://files.pythonhosted.org/packages/0b/66/30f9d8d41049b78bc614213d492792fbcfeb1b28642adf661c42110a7ebd/thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/06/7c/34330a89da55610daa5f245ddce5aab81244321101614751e7537f125133/wasabi-1.1.3-py3-none-any.whl.metadata\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/73/aa/8393344ca7f0e81965febba07afc5cad68335ed0426408d480b861ab915b/srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.5.0,>=0.4.2 from https://files.pythonhosted.org/packages/a4/74/a148b41572656904a39dfcfed3f84dd1066014eed94e209223ae8e9d088d/weasel-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer-slim<1.0.0,>=0.3.0 from https://files.pythonhosted.org/packages/5e/dd/5cbf31f402f1cc0ab087c94d4669cfa55bd1e818688b910631e131d74e75/typer_slim-0.20.0-py3-none-any.whl.metadata\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python312\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gisel\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python312\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gisel\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for blis<1.4.0,>=1.3.0 from https://files.pythonhosted.org/packages/20/07/fb43edc2ff0a6a367e4a94fc39eb3b85aa1e55e24cc857af2db145ce9f0d/blis-1.3.3-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gisel\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<1.0.0,>=0.7.0 from https://files.pythonhosted.org/packages/ae/8a/c4bb04426d608be4a3171efa2e233d2c59a5c8937850c10d098e126df18e/cloudpathlib-0.23.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.5 MB 991.0 kB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/1.5 MB 656.4 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.5 MB 901.1 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.5/14.2 MB 48.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.9/14.2 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.6/14.2 MB 36.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.1/14.2 MB 35.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.3/14.2 MB 42.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.3/14.2 MB 42.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.6/14.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.6/14.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 29.8 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.2/40.2 kB 2.0 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.3/118.3 kB 6.8 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 654.8/654.8 kB 40.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.7 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.7 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 3.0/6.2 MB 95.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.2 MB 71.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 65.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 44.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB ? eta 0:00:00\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wasabi, threadpoolctl, spacy-loggers, spacy-legacy, murmurhash, joblib, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, scikit-learn, preshed, nltk, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 joblib-1.5.2 murmurhash-1.0.15 nltk-3.9.2 preshed-3.0.12 scikit-learn-1.7.2 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 threadpoolctl-3.6.0 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn nltk spacy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc045f88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"NLTK version:\", nltk.__version__)\n",
    "print(\"SpaCy version:\", spacy.__version__)\n",
    "print(\"Gensim:\", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35b6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A soma de a e b é: 30\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 20\n",
    "print(\"A soma de a e b é:\", a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97577235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens de palavras: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.', 'Eles', 'podem', 'realizar', 'muitas', 'tarefas', 'automaticamente', '.']\n",
      "Tokens de sentenças: ['Chatbots estão se tornando cada vez mais populares.', 'Eles podem realizar muitas tarefas automaticamente.']\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares. Eles podem realizar muitas tarefas automaticamente.\"\n",
    "\n",
    "# Tokenização em palavras\n",
    "tokens_palavras = word_tokenize(texto)\n",
    "print(\"Tokens de palavras:\", tokens_palavras)\n",
    "\n",
    "# Tokenização em sentenças\n",
    "tokens_sentencas = sent_tokenize(texto)\n",
    "print(\"Tokens de sentenças:\", tokens_sentencas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# import os\n",
    "# os.system(\"python -m spacy download pt_core_news_sm\")\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenização\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicializando o lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Exemplo de palavras\n",
    "palavras = [\"correndo\", \"correu\", \"corredores\"]\n",
    "\n",
    "# Lematização das palavras\n",
    "lematizadas = [lemmatizer.lemmatize(palavra, pos='v') for palavra in palavras]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "lematizadas = [token.lemma_ for token in doc]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inicializando o stemizador\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Exemplo de palavras\n",
    "palavras = [\"correndo\", \"correu\", \"corredores\"]\n",
    "\n",
    "# Stemização das palavras\n",
    "stems = [stemmer.stem(palavra) for palavra in palavras]\n",
    "print(\"Stems das palavras:\", stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Carregar stopwords para o idioma portugues\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Removendo stopwords\n",
    "tokens_sem_stopwords = [palavra for palavra in word_tokenize(texto) if palavra.lower() not in stop_words]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8660272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download pt_core_news_sm\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Removendo stopwords\n",
    "tokens_sem_stopwords = [token.text for token in doc if no\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texto = \"Entre em contato em exemplo@email.com ou suporte@outroemail.com.\"\n",
    "padrao = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-] +\\.[A-Z|a-z]{2,}\\b'\n",
    "emails = re.findall(padrao, texto)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "padrao_data = r'\\b\\d{2}/\\d{2}/\\d{4}\\b'\n",
    "datas_teste = [\"31/12/2020\", \"1/1/2021\", \"2023-05-10\", \"25/06/2025 10:00\"]\n",
    "\n",
    "for data in datas_teste:\n",
    "if re.match(padrao_data, data):\n",
    "print(f\"'{data}' é uma data válida no formato DD/MM/AAAA.\")\n",
    "else:\n",
    "print(f\"'{data}' não é uma data válida no formato DD/MM/AAAA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93470058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "padrao_comando = r'^/\\w+'\n",
    "comandos_teste = [\"/ajuda\", \"/iniciar\", \"ajuda\", \"iniciar/\"]\n",
    "\n",
    "for comando in comandos_teste:\n",
    "    if re.match(padrao_comando, comando):\n",
    "        print(f\"'{comando}' é um comando válido.\")\n",
    "    else:\n",
    "        print(f\"'{comando}' não é um comando válido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texto = \"Olá, como vai você?\"\n",
    "tokens = re.split(r'\\W+', texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texto_html = \"<p>Este é um parágrafo com <b>texto em negrito</b>.</p>\"\n",
    "texto_limpo = re.sub(r'<[^>]+>', '', texto_html)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ab4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de texto\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\"]\n",
    "\n",
    "# Tokenização simples\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "# Flatten para obter todas as palavras\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "# Remover duplicatas\n",
    "vocab = list(set(all_words))\n",
    "\n",
    "# Criar matriz de índices das palavras para cada frase\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "corpus_idx = [[word_to_idx[word] for word in sentence] for sentence in tokenized_corpus]\n",
    "\n",
    "# Ajustar o encoder para o vocabulário\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform(\n",
    "        np.array(vocab).reshape(-1, 1))\n",
    "\n",
    "print(\"Vocabulário:\", vocab)\n",
    "print(\"One-Hot Encoding:\\n\", one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Exemplo de corpus\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n",
    "\n",
    "# Criação do vetor de contagem (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulário:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Exemplo de corpus\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n",
    "\n",
    "# Criação do vetor TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulário:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bdfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Exemplo de corpus e rótulos\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu odeio bugs\", \"A vida é bela\", \"Eu odeio erros\"]\n",
    "labels = [1, 1, 0, 1, 0]  # 1: Sentimento Positivo, 0: Sentimento Negativo\n",
    "\n",
    "# Divisão dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "# Criação do pipeline TF-IDF + Classificador Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predição no conjunto de teste\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(\"Relatório de Classificação:\\n\", metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim nltk\t\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=0)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector = model.wv['economy']\n",
    "print(\"Vetor para 'economy':\", vector)\n",
    "\n",
    "# Encontrando palavras semelhantes\n",
    "similar_words = model.wv.most_similar('economy')\n",
    "print(\"Palavras semelhantes a 'economy':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0945d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim nltk\t\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Analogias: \"rei\" - \"homem\" + \"mulher\" = ?\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"Resultado da analogia:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "# pip install gensim\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_filename = \"glove.6B.zip\"\n",
    "txt_filename = \"glove.6B.50d.txt\"\n",
    "\n",
    "# Baixar o arquivo zip se não existir\n",
    "if not os.path.exists(zip_filename):\n",
    "\tprint(\"Baixando embeddings GloVe 6B...\")\n",
    "\twith requests.get(url, stream=True) as r:\n",
    "\t\tr.raise_for_status()\n",
    "\t\twith open(zip_filename, \"wb\") as f:\n",
    "\t\t\tfor chunk in r.iter_content(chunk_size=8192):\n",
    "\t\t\t\tf.write(chunk)\n",
    "\tprint(\"Download concluído.\")\n",
    "\n",
    "# Extrair o arquivo txt se não existir\n",
    "if not os.path.exists(txt_filename):\n",
    "\tprint(\"Extraindo glove.6B.50d.txt...\")\n",
    "\twith zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "\t\tzip_ref.extract(txt_filename)\n",
    "\tprint(\"Extração concluída.\")\n",
    "\n",
    "# Carregar embeddings GloVe\n",
    "glove_model = KeyedVectors.load_word2vec_format(txt_filename, binary=False)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector_glove = glove_model['economy']\n",
    "print(\"Vetor para 'economy' com GloVe:\", vector_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953644b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo FastText\n",
    "fasttext_model = gensim.models.FastText(sentences, vector_size=100, window=5, min_count=5)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector_fasttext = fasttext_model.wv['economy']\n",
    "print(\"Vetor para 'economy' com FastText:\", vector_fasttext)\n",
    "\n",
    "# Encontrando palavras semelhantes\n",
    "similar_words_ft = fasttext_model.wv.most_similar('economy')\n",
    "print(\"Palavras semelhantes a 'economy' com FastText:\", similar_words_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf3783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
