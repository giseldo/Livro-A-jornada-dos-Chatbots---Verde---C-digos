{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84578ef",
   "metadata": {},
   "source": [
    "#  Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83e09c",
   "metadata": {},
   "source": [
    "## Código-fonte 3.1\n",
    "Comando do terminal para instalar algumas das bibliotecas\n",
    "necessárias do Python via pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99f2c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: nltk in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: gensim in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\projetos livros\\livro chatbots com python códigos\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn nltk spacy gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b31ca7",
   "metadata": {},
   "source": [
    "## Código-fonte 3.2\n",
    "Exibe as versões das bibliotecas instaladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc045f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.5\n",
      "Pandas version: 2.3.3\n",
      "Scikit-learn version: 1.7.2\n",
      "NLTK version: 3.9.2\n",
      "SpaCy version: 3.8.11\n",
      "Gensim: 4.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"NLTK version:\", nltk.__version__)\n",
    "print(\"SpaCy version:\", spacy.__version__)\n",
    "print(\"Gensim:\", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ab4ee",
   "metadata": {},
   "source": [
    "## Código-fonte 3.5 \n",
    "Código Python para somar dois números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35b6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A soma de a e b é: 30\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 20\n",
    "print(\"A soma de a e b é:\", a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f7c28",
   "metadata": {},
   "source": [
    "## Código-fonte 3.6\n",
    "Este código demonstra como separar um texto em suas palavras e\n",
    "sentenças usando NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97577235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens de palavras: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.', 'Eles', 'podem', 'realizar', 'muitas', 'tarefas', 'automaticamente', '.']\n",
      "Tokens de sentenças: ['Chatbots estão se tornando cada vez mais populares.', 'Eles podem realizar muitas tarefas automaticamente.']\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares. Eles podem realizar muitas tarefas automaticamente.\"\n",
    "\n",
    "# Tokenização em palavras\n",
    "tokens_palavras = word_tokenize(texto)\n",
    "print(\"Tokens de palavras:\", tokens_palavras)\n",
    "\n",
    "# Tokenização em sentenças\n",
    "tokens_sentencas = sent_tokenize(texto)\n",
    "print(\"Tokens de sentenças:\", tokens_sentencas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c1190",
   "metadata": {},
   "source": [
    "## Código-fonte 3.7\n",
    "Utiliza a biblioteca SpaCy para processamento de linguagem natural\n",
    "em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72acb3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/13.0 MB ? eta -:--:--\n",
      "     ----------------------------- ---------- 9.7/13.0 MB 46.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 13.0/13.0 MB 42.8 MB/s  0:00:00\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a715d9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# import os\n",
    "# os.system(\"python -m spacy download pt_core_news_sm\")\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenização\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497f3a7",
   "metadata": {},
   "source": [
    "## Código-fonte 3.8\n",
    "Utiliza a biblioteca NLTK para realizar lematização de palavras em\n",
    "português. Ele define uma lista de palavras e aplica o lematizador a cada uma delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01fd44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras lematizadas: ['correndo', 'correu', 'corredores']\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicializando o lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Exemplo de palavras}\n",
    "palavras = [\"correndo\", \"correu\", \"corredores\"]\n",
    "\n",
    "# Lematização das palavras\n",
    "lematizadas = [lemmatizer.lemmatize(palavra, pos='v') for palavra in palavras]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a93183",
   "metadata": {},
   "source": [
    "## Código-fonte 3.9\n",
    "Mesmo exemplo do código anterior porém utilizando a biblioteca\n",
    "Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e419def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\n",
      "Palavras lematizadas: ['chatbots', 'estar', 'se', 'tornar', 'cada', 'vez', 'mais', 'popular', '.']\n"
     ]
    }
   ],
   "source": [
    "#pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "lematizadas = [token.lemma_ for token in doc]\n",
    "print(\"Palavras lematizadas:\", lematizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da10cff",
   "metadata": {},
   "source": [
    "## Código-fonte 3.10\n",
    "Utiliza a biblioteca NLTK para realizar a stemizacao de palavras. Ele\n",
    "define uma lista de palavras e aplica o stemmer para obter os radicais dessas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d38cc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems das palavras: ['correndo', 'correu', 'corredor']\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inicializando o stemizador\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Exemplo de palavras\n",
    "palavras = [\"correndo\", \"correu\", \"corredores\"]\n",
    "\n",
    "# Stemização das palavras\n",
    "stems = [stemmer.stem(palavra) for palavra in palavras]\n",
    "print(\"Stems das palavras:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c10cce",
   "metadata": {},
   "source": [
    "## Código-fonte 3.11 \n",
    "Demonstra como extrair o radical de palavras usando NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cad6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords: ['Chatbots', 'tornando', 'cada', 'vez', 'populares', '.']\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Carregar stopwords para o idioma portugues\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Removendo stopwords\n",
    "tokens_sem_stopwords = [palavra for palavra in word_tokenize(texto) if palavra.lower() not in stop_words]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfcfb5",
   "metadata": {},
   "source": [
    "## Código-fonte 3.12\n",
    "Demonstra como tokenizar um texto e filtrar stopwords usando SpaCy\n",
    "em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8660272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\n",
      "Texto sem stopwords: ['Chatbots', 'tornando', 'populares', '.']\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download pt_core_news_sm\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Chatbots estão se tornando cada vez mais populares.\"\n",
    "\n",
    "# Processando o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Removendo stopwords\n",
    "tokens_sem_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Texto sem stopwords:\", tokens_sem_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c829d0",
   "metadata": {},
   "source": [
    "## Código-fonte 3.13\n",
    "Extração de e-mails com regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "751c33b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texto = \"Entre em contato em exemplo@email.com ou suporte@outroemail.com.\"\n",
    "padrao = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-] +\\.[A-Z|a-z]{2,}\\b'\n",
    "emails = re.findall(padrao, texto)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24c50e",
   "metadata": {},
   "source": [
    "## Código-fonte 3.14\n",
    "Validação de datas com regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'31/12/2020' é uma data válida no formato DD/MM/AAAA.\n",
      "'1/1/2021' não é uma data válida no formato DD/MM/AAAA.\n",
      "'2023-05-10' não é uma data válida no formato DD/MM/AAAA.\n",
      "'25/06/2025 10:00' é uma data válida no formato DD/MM/AAAA.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "padrao_data = r'\\b\\d{2}/\\d{2}/\\d{4}\\b'\n",
    "datas_teste = [\"31/12/2020\", \"1/1/2021\", \"2023-05-10\", \"25/06/2025 10:00\"]\n",
    "\n",
    "for data in datas_teste:\n",
    "  if re.match(padrao_data, data):\n",
    "    print(f\"'{data}' é uma data válida no formato DD/MM/AAAA.\")\n",
    "  else:\n",
    "    print(f\"'{data}' não é uma data válida no formato DD/MM/AAAA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a20a1f",
   "metadata": {},
   "source": [
    "## Código-fonte 3.15\n",
    "Análise de comandos com regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93470058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/ajuda' é um comando válido.\n",
      "'/iniciar' é um comando válido.\n",
      "'ajuda' não é um comando válido.\n",
      "'iniciar/' não é um comando válido.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "padrao_comando = r'^/\\w+'\n",
    "comandos_teste = [\"/ajuda\", \"/iniciar\", \"ajuda\", \"iniciar/\"]\n",
    "\n",
    "for comando in comandos_teste:\n",
    "    if re.match(padrao_comando, comando):\n",
    "        print(f\"'{comando}' é um comando válido.\")\n",
    "    else:\n",
    "        print(f\"'{comando}' não é um comando válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cec76f",
   "metadata": {},
   "source": [
    "## Código-fonte 3.16\n",
    "Tokenização simples com regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83af68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Olá', 'como', 'vai', 'você', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texto = \"Olá, como vai você?\"\n",
    "tokens = re.split(r'\\W+', texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5628667",
   "metadata": {},
   "source": [
    "## Código-fonte 3.17\n",
    "Limpeza de texto removendo tags HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0962944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este é um parágrafo com texto em negrito.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texto_html = \"<p>Este é um parágrafo com <b>texto em negrito</b>.</p>\"\n",
    "texto_limpo = re.sub(r'<[^>]+>', '', texto_html)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955e45",
   "metadata": {},
   "source": [
    "## Código-fonte 3.18\n",
    "Transforma palavras de um pequeno corpus de frases em vetores\n",
    "one-hot usando o OneHotEncoder do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1ab4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['programação', 'amo', 'é', 'A', 'divertida', 'Eu']\n",
      "One-Hot Encoding:\n",
      " [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de texto\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\"]\n",
    "\n",
    "# Tokenização simples\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "# Flatten para obter todas as palavras\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "# Remover duplicatas\n",
    "vocab = list(set(all_words))\n",
    "\n",
    "# Criar matriz de índices das palavras para cada frase\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "corpus_idx = [[word_to_idx[word] for word in sentence] for sentence in tokenized_corpus]\n",
    "\n",
    "# Ajustar o encoder para o vocabulário\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform(\n",
    "        np.array(vocab).reshape(-1, 1))\n",
    "\n",
    "print(\"Vocabulário:\", vocab)\n",
    "print(\"One-Hot Encoding:\\n\", one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8b0d7",
   "metadata": {},
   "source": [
    "## Código-fonte 3.19\n",
    "Utiliza a biblioteca scikit-learn para transformar um conjunto de\n",
    "frases (corpus) em uma matriz BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18b4dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['amo' 'divertida' 'eu' 'programação' 'vida']\n",
      "Bag of Words Matrix:\n",
      " [[1 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Exemplo de corpus\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n",
    "\n",
    "# Criação do vetor de contagem (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulário:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0c608",
   "metadata": {},
   "source": [
    "## Código-fonte 3.20\n",
    "Técnica comum em processamento de linguagem natural para\n",
    "converter texto em dados numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32be8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['amo' 'divertida' 'eu' 'programação' 'vida']\n",
      "TF-IDF Matrix:\n",
      " [[0.57735027 0.         0.57735027 0.57735027 0.        ]\n",
      " [0.         0.79596054 0.         0.60534851 0.        ]\n",
      " [0.51785612 0.         0.51785612 0.         0.68091856]]\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Exemplo de corpus\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n",
    "\n",
    "# Criação do vetor TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulário:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67f24d",
   "metadata": {},
   "source": [
    "## Código-fonte 3.21\n",
    "Treino e avaliação de um modelo simples de classificação de\n",
    "sentimentos em textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74bdfa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projetos Livros\\Livro Chatbots com Python Códigos\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Projetos Livros\\Livro Chatbots com Python Códigos\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Projetos Livros\\Livro Chatbots com Python Códigos\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Exemplo de corpus e rótulos\n",
    "corpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu odeio bugs\", \"A vida é bela\", \"Eu odeio erros\"]\n",
    "labels = [1, 1, 0, 1, 0]  # 1: Sentimento Positivo, 0: Sentimento Negativo\n",
    "\n",
    "# Divisão dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "# Criação do pipeline TF-IDF + Classificador Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predição no conjunto de teste\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(\"Relatório de Classificação:\\n\", metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56f079",
   "metadata": {},
   "source": [
    "## Código-fonte 3.22\n",
    "Treina um modelo de Word2Vec usando o corpus news do Brown\n",
    "(disponível no NLTK) para gerar representações vetoriais de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d594249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor para 'economy': [-0.04815609  0.01639045 -0.01870394 -0.01663649 -0.02984198 -0.12746063\n",
      "  0.0465952   0.15816896 -0.08479835 -0.06942044  0.06977347 -0.07013117\n",
      " -0.07630205 -0.01674319  0.02286813 -0.06601285  0.03113458 -0.02182284\n",
      " -0.04738552 -0.09850634  0.04903988  0.03314869  0.13632214 -0.0494479\n",
      " -0.07217429  0.08378216 -0.07366972  0.05642854 -0.01578441 -0.03794022\n",
      "  0.08537542 -0.06744939  0.01029237 -0.04564841  0.0032799   0.11974443\n",
      "  0.02657936 -0.02911894 -0.05277169 -0.04892118  0.03181053 -0.06764908\n",
      " -0.04648553 -0.00583027  0.06949913 -0.07281978 -0.02848032 -0.01343263\n",
      "  0.01498829  0.03713196  0.01046777 -0.03792575 -0.02187441 -0.02401222\n",
      " -0.05076339 -0.00194816  0.06511394 -0.06000043  0.01305613 -0.01043953\n",
      " -0.04224693  0.0417886   0.01061732  0.0013354  -0.05156695  0.11047818\n",
      " -0.01610061  0.02809229 -0.05760334  0.04505866 -0.08252972  0.06258633\n",
      "  0.10864721  0.0203197   0.02080672  0.04995502 -0.04221418 -0.00029486\n",
      " -0.07728022  0.01529829 -0.08624682  0.01126853 -0.00970634  0.06548135\n",
      " -0.02008096 -0.00704846 -0.01540325  0.05973321  0.03077581  0.03629608\n",
      "  0.0849342   0.04814466 -0.01529912  0.00426517  0.14912936  0.05115908\n",
      "  0.03227679 -0.07126185 -0.03162598  0.00968701]\n",
      "Palavras semelhantes a 'economy': [('U.S.', 0.9952619671821594), ('Jones', 0.9952570796012878), ('open', 0.9952099919319153), ('The', 0.9951969385147095), ('League', 0.9951803684234619), ('asked', 0.9951463341712952), ('announced', 0.9951319098472595), ('Robert', 0.9951086640357971), ('system', 0.9950973391532898), ('were', 0.9950648546218872)]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim nltk\t\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=0)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector = model.wv['economy']\n",
    "print(\"Vetor para 'economy':\", vector)\n",
    "\n",
    "# Encontrando palavras semelhantes\n",
    "similar_words = model.wv.most_similar('economy')\n",
    "print(\"Palavras semelhantes a 'economy':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e47eb0",
   "metadata": {},
   "source": [
    "## Código-fonte 3.23\n",
    "Treina um modelo de Word2Vec e faz uma analogia de palavras:\n",
    "”king- ”man”+ ”woman”, buscando o termo mais próximo desse contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0945d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado da analogia: [('Mercer', 0.8664076924324036), ('La.', 0.8649832606315613), ('questions', 0.8608457446098328), ('Fighters', 0.8598513007164001), ('discuss', 0.8593618869781494), ('Art', 0.8570495247840881), ('Socialist', 0.8569803833961487), ('seldom', 0.8569306135177612), ('1953', 0.8563147783279419), ('relatively', 0.8557248115539551)]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim nltk\t\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Analogias: \"rei\" - \"homem\" + \"mulher\" = ?\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"Resultado da analogia:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972eb2b3",
   "metadata": {},
   "source": [
    "## Código-fonte 3.24\n",
    "Faz o download dos embeddings de palavras GloVe e extrai o arquivo\n",
    "necessário, carrega os vetores e obtém o vetor de características para a palavra\n",
    "economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b2e901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor para 'economy' com GloVe: [-1.2027e-01 -7.2505e-01  8.7014e-01 -6.3944e-01  1.7259e-01 -3.5168e-01\n",
      " -6.5425e-01 -7.2757e-01 -2.2327e-01  1.3200e-01  4.2210e-01 -2.1129e-01\n",
      " -3.1140e-01  4.7728e-01  3.1158e-01  6.4071e-01  2.2868e-01 -1.8580e-01\n",
      "  8.0219e-01  6.9265e-03  3.6053e-01 -7.4774e-01 -8.9363e-01 -6.6631e-01\n",
      "  1.0789e+00 -1.3036e+00  2.8634e-03  3.6411e-01  5.2839e-01  1.7480e+00\n",
      "  3.7130e+00  7.0001e-01  9.2982e-01 -1.7352e-01 -8.3904e-01 -4.2105e-01\n",
      " -1.4294e+00  5.7824e-01  5.8892e-01 -8.5238e-01 -1.7313e+00  4.5091e-02\n",
      "  4.9483e-01 -1.0151e+00  8.9959e-02  4.6090e-01  1.7585e-03  6.2182e-01\n",
      "  1.1893e+00  8.4410e-02]\n"
     ]
    }
   ],
   "source": [
    "# pip install requests\n",
    "# pip install gensim\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_filename = \"glove.6B.zip\"\n",
    "txt_filename = \"glove.6B.50d.txt\"\n",
    "\n",
    "# Baixar o arquivo zip se não existir\n",
    "if not os.path.exists(zip_filename):\n",
    "\tprint(\"Baixando embeddings GloVe 6B...\")\n",
    "\twith requests.get(url, stream=True) as r:\n",
    "\t\tr.raise_for_status()\n",
    "\t\twith open(zip_filename, \"wb\") as f:\n",
    "\t\t\tfor chunk in r.iter_content(chunk_size=8192):\n",
    "\t\t\t\tf.write(chunk)\n",
    "\tprint(\"Download concluído.\")\n",
    "\n",
    "# Extrair o arquivo txt se não existir\n",
    "if not os.path.exists(txt_filename):\n",
    "\tprint(\"Extraindo glove.6B.50d.txt...\")\n",
    "\twith zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "\t\tzip_ref.extract(txt_filename)\n",
    "\tprint(\"Extração concluída.\")\n",
    "\n",
    "# Carregar embeddings GloVe\n",
    "glove_model = KeyedVectors.load_word2vec_format(txt_filename, binary=False, no_header=True)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector_glove = glove_model['economy']\n",
    "print(\"Vetor para 'economy' com GloVe:\", vector_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d694a4b",
   "metadata": {},
   "source": [
    "## Código-fonte 3.25\n",
    "Este código treina um modelo de linguagem FastText usando o\n",
    "corpus news do Brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "953644b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor para 'economy' com FastText: [-0.07913742  0.25233224 -0.07079501 -0.23172215  0.06874844  0.03965204\n",
      " -0.00313755  0.15882023  0.18099971 -0.10419944  0.20775302  0.07710753\n",
      " -0.19060253  0.2600621  -0.3090463  -0.19312479 -0.06089186 -0.04025605\n",
      " -0.26604268 -0.0498223  -0.23428224  0.19358292  0.02633066 -0.10356053\n",
      "  0.00286019 -0.22827028 -0.24923463  0.03381338 -0.0231407   0.00647127\n",
      " -0.01258843  0.02028346  0.44021755 -0.17614518  0.0061322   0.19301128\n",
      "  0.07359346  0.1785287  -0.18454622 -0.09717848  0.17560244 -0.06990219\n",
      "  0.11357515 -0.23170672 -0.20930912 -0.1475308  -0.06379472  0.16089591\n",
      " -0.0184421  -0.09846582  0.09060253 -0.22754261  0.10772247 -0.288139\n",
      "  0.09074466  0.00781813 -0.17222355 -0.13406675 -0.21547098 -0.10958227\n",
      " -0.14449959 -0.18664409  0.11154978  0.26387405  0.08376542  0.32949618\n",
      " -0.1366519  -0.09476344  0.22028679  0.07690582 -0.04966376  0.14929171\n",
      "  0.18170822 -0.19063792  0.0154007  -0.12020912  0.08991286 -0.01368116\n",
      " -0.03464435  0.25567022 -0.02322021 -0.1411807  -0.1670661   0.01949121\n",
      " -0.18533234 -0.13663362  0.21869674  0.15269728 -0.0844155   0.20122884\n",
      "  0.00606576  0.01067166  0.09974551  0.00461376 -0.0247305   0.17751965\n",
      "  0.07064788 -0.19173346 -0.028597    0.07000184]\n",
      "Palavras semelhantes a 'economy' com FastText: [('economic', 0.9999739527702332), ('recommended', 0.9999644160270691), ('concerned', 0.9999639391899109), ('important', 0.9999633431434631), ('defensive', 0.9999633431434631), ('recommendations', 0.9999632835388184), ('talent', 0.9999631643295288), ('native', 0.9999629855155945), ('recovery', 0.999962568283081), ('possibility', 0.999962568283081)]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Carregar o corpus de exemplo (Brown corpus)\n",
    "sentences = brown.sents(categories='news')\n",
    "\n",
    "# Treinamento do modelo FastText\n",
    "fasttext_model = gensim.models.FastText(sentences, vector_size=100, window=5, min_count=5)\n",
    "\n",
    "# Obtenção de vetor para uma palavra\n",
    "vector_fasttext = fasttext_model.wv['economy']\n",
    "print(\"Vetor para 'economy' com FastText:\", vector_fasttext)\n",
    "\n",
    "# Encontrando palavras semelhantes\n",
    "similar_words_ft = fasttext_model.wv.most_similar('economy')\n",
    "print(\"Palavras semelhantes a 'economy' com FastText:\", similar_words_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
