{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Carregar o tokenizer e o modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained( 'bert-base-uncased')\n",
    "\n",
    "# Entrada de exemplo\n",
    "input_text = \"Chatbots são muito úteis para automação.\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Predição\n",
    "outputs = model(**input_ids)\n",
    "probs = softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(\"Probabilidades de classe:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Carregar o tokenizer e o modelo BERT pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\n",
    "model = BertModel.from_pretrained( 'bert-base-uncased')\n",
    "\n",
    "# Exemplo de texto\n",
    "texto = \"Machine learning is fascinating.\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer(texto, return_tensors='pt')['input_ids']\n",
    "\n",
    "# Obtenção das representações do modelo BERT\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(\"Representações BERT:\", last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Carregar o tokenizer e o modelo GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Entrada de exemplo\n",
    "input_text = \"Chatbots modernos podem\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Geração de texto\n",
    "outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificação e exibição do texto gerado\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto gerado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f41d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Carregar o tokenizer e o modelo GPT-2 pré-treinado\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Exemplo de texto\n",
    "input_text = \"In the future, artificial intelligence will\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Geração de texto\n",
    "outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificação e exibição do texto gerado\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto gerado:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]\n",
    "# pip install sentencepiece\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Carregar o tokenizer e o modelo T5\n",
    "tokenizer = T5Tokenizer.from_pretrained( 't5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained( 't5-small')\n",
    "\n",
    "# Entrada de exemplo\n",
    "input_text = \"translate English to German: The weather is nice today.\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Geração de texto\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "# Decodificação e exibição do texto gerado\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Tradução gerada:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "# Carregar o tokenizer e o modelo XLNet\n",
    "tokenizer = XLNetTokenizer.from_pretrained( 'xlnet-base-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained( 'xlnet-base-cased')\n",
    "\n",
    "# Entrada de exemplo\n",
    "input_text = \"Natural Language Processing is\"\n",
    "\n",
    "# Tokenização\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Geração de texto\n",
    "outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificação e exibição do texto gerado\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Texto gerado com XLNet:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets\n",
    "# pip install transformers[torch]\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Carregar o dataset IMDb\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Carregar o tokenizer e o modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained( 'bert-base-uncased')\n",
    "\n",
    "# Tokenizar os dados\n",
    "def tokenize_function(examples):\n",
    "return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Definir argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir='./results',\n",
    "\tlearning_rate=2e-5,\n",
    "\treport_to=\"none\",\n",
    "\tper_device_train_batch_size=16,\n",
    "\tper_device_eval_batch_size=16,\n",
    "\tnum_train_epochs=3,\n",
    "\tweight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=tokenized_datasets['train'],\n",
    "eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Avaliar o modelo\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Resultado da Avaliação: {eval_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Carregar o tokenizer e o modelo para as consultas (questions)\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
    "\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Carregar o tokenizer e o modelo para os contextos (passages)\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Exemplo de consulta\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "\n",
    "# Codificar a consulta\n",
    "query_input = question_tokenizer(query, return_tensors=\"pt\")\n",
    "query_embedding = question_encoder(**query_input).pooler_output\n",
    "\n",
    "# Exemplo de passagens\n",
    "passages = [\n",
    "    \"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.\",\n",
    "    \"It allows for more accurate and contextually relevant answers by consulting external knowledge bases.\",\n",
    "    \"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.\"\n",
    "]\n",
    "\n",
    "# Codificar as passagens\n",
    "passage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "passage_embeddings = context_encoder(\n",
    "    **passage_inputs).pooler_output\n",
    "\n",
    "# Calcular similaridade e selecionar a passagem mais relevante\n",
    "similarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\n",
    "best_passage_index = torch.argmax(similarity_scores, dim=1).item()\n",
    "best_passage = passages[best_passage_index]\n",
    "\n",
    "print(\"Passagem mais relevante:\", best_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Carregar o tokenizer e o modelo para as consultas (questions)\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
    "\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Carregar o tokenizer e o modelo para os contextos (passages)\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Exemplo de consulta\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "\n",
    "# Codificar a consulta\n",
    "query_input = question_tokenizer(query, return_tensors=\"pt\")\n",
    "query_embedding = question_encoder(**query_input).pooler_output\n",
    "\n",
    "# Exemplo de passagens\n",
    "passages = [\n",
    "    \"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.\",\n",
    "    \"It allows for more accurate and contextually relevant answers by consulting external knowledge bases.\",\n",
    "    \"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.\"\n",
    "]\n",
    "\n",
    "# Codificar as passagens\n",
    "passage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "passage_embeddings = context_encoder\n",
    "   **passage_inputs).pooler_output\n",
    "\n",
    "# Calcular similaridade e selecionar a passagem mais relevante\n",
    "similarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\n",
    "best_passage_index = torch.argmax(similarity_scores, dim=1).item()\n",
    "best_passage = passages[best_passage_index]\n",
    "\n",
    "# Carregar o tokenizer e o modelo BART\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\n",
    "\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\n",
    "\"facebook/bart-large\")\n",
    "\n",
    "# Concatenar a consulta com a passagem relevante\n",
    "input_text = query + \" \" + best_passage\n",
    "\n",
    "# Codificar e gerar resposta\n",
    "input_ids = bart_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "generated_ids = bart_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "generated_text = bart_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Resposta gerada:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720855b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "from google.colab import userdata\n",
    "userdata.get('HF_TOKEN')\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=userdata.get('HF_TOKEN'))\n",
    "\n",
    "# Carregar o tokenizer e o modelo LLaMA\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "# Texto de entrada\n",
    "input_text = \"Artificial intelligence is transforming the world of\"\n",
    "\n",
    "# Tokenizar e gerar texto\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Texto gerado:\", generated_text)\n",
    "\\end{lstlisting} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3249863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model='gpt2')\n",
    "input = 'Olá, como vai você?'\n",
    "output = pipe(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input= \"Olá, como vai você?\"\t\n",
    "token_id = tokenizer(input)\n",
    "print(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", \n",
    "\tmodel_provider=\"openai\", \t\t\t\t\t\topenai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "user_message = HumanMessage(content=\"Quem é o presidente do Brasil?\")\n",
    "\n",
    "response = model.invoke([user_message])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para instalação acesse https://github.com/Mangaba-ai/mangaba_ai\n",
    "from mangaba import Team, Agent\n",
    "\n",
    "# Criar uma equipe de agentes\n",
    "monitor = Team(\"Autorregulação\")\n",
    "\n",
    "# Adicionar agentes especializados\n",
    "planejador = Agent(\"Plano de Estudo\", role=\"Gera Plano de Estudo com cronograma baseado no tempo disponível do estudante\")\n",
    "apoio = Agent(\"Suporte Emocional\", role=\"Motiva o estudante\")\n",
    "pesquisador = Agent(\"Guia de Estudo\", role=\"Busca materiais didáticos para o estudante\")\n",
    "\n",
    "# Adicionar agentes à equipe\n",
    "monitor.add_agents([pesquisador, apoio, planejador])\n",
    "\n",
    "# Definir uma tarefa complexa\n",
    "result = monitor.solve(\n",
    "  \"Pesquise sobre Redes Neurais Artificiais\"\n",
    ")\n",
    "\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31025a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hugging Face Transformers.}]\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DPRQuestionEncoder, DPRContextEncoder\n",
    "\n",
    "# Carregar o tokenizer e o modelo LLaMA\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"meta-llama/LLaMA-7B\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "\"meta-llama/LLaMA-7B\")\n",
    "\n",
    "# Configurar DPR para recuperação de passagens\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained( \"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este código deve ser executado na sequencia, depois do código anterior.\n",
    "import torch\n",
    "\n",
    "def retrieve_relevant_passage(query, passages):\n",
    "\tquery_input = question_tokenizer(query, return_tensors=\"pt\")\n",
    "\tquery_embedding = question_encoder(\n",
    "       **query_input).pooler_output\n",
    "\n",
    "\tpassage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\tpassage_embeddings = context_encoder(\n",
    "      **passage_inputs).pooler_output\n",
    "\n",
    "\tsimilarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\n",
    "\tbest_passage_index = torch.argmax(similarity_scores, dim=1).item()\n",
    "\n",
    "\treturn passages[best_passage_index]\n",
    "\n",
    "# Exemplo de passagens\n",
    "passages = [\n",
    "\t\"LLaMA é um modelo de linguagem desenvolvido pela Meta AI.\",\n",
    "\t\"RAG combina recuperação de informações com geração de texto.\",\n",
    "\t\"GPT-3 é um dos maiores modelos de linguagem disponíveis.\"\n",
    "]\n",
    "\n",
    "# Entrada do usuário\n",
    "user_input = \"O que é LLaMA?\"\n",
    "\n",
    "# Recuperar a passagem mais relevante\n",
    "relevant_passage = retrieve_relevant_passage(user_input, passages)\n",
    "\n",
    "print(f\"Passagem mais relevante: {relevant_passage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar a consulta com a passagem relevante\n",
    "input_text = user_input + \" \" + relevant_passage\n",
    "\n",
    "# Geração da resposta\n",
    "input_ids = llama_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "generated_ids = llama_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "response = llama_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Resposta do chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0af02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264b8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f53ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83ccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574bda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a6b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
